{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYGUy3XPaKD9St9eINr0SZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zkibnksmh/MachineLearningTasks/blob/main/UASML/Exercise_07_PyTorch_Experiment_Tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDsiktALkHma"
      },
      "outputs": [],
      "source": [
        "#Zaky Ibnu Kusumah\n",
        "#1103204213\n",
        "#TK4404"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**07. PyTorch Experiment Tracking**"
      ],
      "metadata": {
        "id": "rljElTHikMkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Get various imports and helper functions**"
      ],
      "metadata": {
        "id": "P4zXRCFFnWAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQrxWnzClnHU",
        "outputId": "3b4db9c6-21c1-4eca-b835-ced9a46c6d8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "\u001b[31m  ERROR: HTTP error 403 while getting https://download.pytorch.org/whl/nightly/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from https://download.pytorch.org/whl/nightly/cu113/nvidia-cuda-nvrtc-cu12/)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not install requirement nvidia-cuda-nvrtc-cu12==12.1.105 from https://download.pytorch.org/whl/nightly/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch) because of HTTP error 403 Client Error: Forbidden for url: https://download.pytorch.org/whl/nightly/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl for URL https://download.pytorch.org/whl/nightly/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from https://download.pytorch.org/whl/nightly/cu113/nvidia-cuda-nvrtc-cu12/)\u001b[0m\u001b[31m\n",
            "\u001b[0mtorch version: 2.1.0+cu121\n",
            "torchvision version: 0.16.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Penjelasan Kode Pemeriksaan Versi PyTorch dan torchvision:\n",
        "\n",
        "Kode tersebut digunakan untuk memeriksa versi PyTorch dan torchvision yang terpasang pada environment saat ini. Jika versi yang terpasang tidak memenuhi persyaratan (PyTorch 1.12+ dan torchvision 0.13+), maka kode akan menginstall versi nightly terbaru dari PyTorch dan torchvision.\n",
        "\n",
        "**Langkah-langkah Kode:**\n",
        "\n",
        "1. Coba impor modul `torch` dan `torchvision`.\n",
        "2. Gunakan pernyataan `assert` untuk memastikan versi `torch` >= 1.12 dan `torchvision` >= 0.13. Jika tidak memenuhi, lempar pesan kesalahan.\n",
        "3. Jika berhasil melewati pemeriksaan, tampilkan versi `torch` dan `torchvision` yang terpasang.\n",
        "4. Jika ada kesalahan atau versi tidak memenuhi persyaratan, cetak pesan bahwa versi tidak sesuai dan instal versi nightly terbaru dari `PyTorch` dan `torchvision`.\n",
        "5. Setelah instalasi selesai, impor kembali modul `torch` dan `torchvision` dan tampilkan versi yang telah terpasang.\n",
        "\n",
        "Tujuan dari kode ini adalah memastikan bahwa notebook dapat berjalan dengan versi PyTorch dan torchvision yang memenuhi persyaratan untuk menjalankan kode dengan API yang diperbarui.\n"
      ],
      "metadata": {
        "id": "JO7znm-omuni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available(): Fungsi ini memeriksa apakah sistem memiliki GPU yang dapat digunakan menggunakan modul CUDA dari PyTorch. Jika tersedia, fungsi ini mengembalikan True, dan jika tidak, mengembalikan False.\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\":\n",
        "# Pada baris ini, menggunakan ekspresi kondisional (ternary operator) untuk menetapkan nilai variabel device.\n",
        "# Jika torch.cuda.is_available() bernilai True, maka device diatur ke \"cuda\" (menggunakan GPU), dan jika False, diatur ke \"cpu\" (menggunakan CPU).\n",
        "\n",
        "# device:\n",
        "# Baris ini mencetak nilai variabel device, yang dapat berupa \"cuda\" jika GPU tersedia atau \"cpu\" jika tidak.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "v6_L6RvcngX7",
        "outputId": "01ca50e4-cd19-4ae5-d519-f5e942a9dce9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygRv4yXnpAJT",
        "outputId": "679fbc1b-820a-4f5b-a13a-445bba092251"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4036, done.\u001b[K\n",
            "remote: Counting objects: 100% (1224/1224), done.\u001b[K\n",
            "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
            "remote: Total 4036 (delta 1068), reused 1078 (delta 996), pack-reused 2812\u001b[K\n",
            "Receiving objects: 100% (4036/4036), 651.02 MiB | 30.14 MiB/s, done.\n",
            "Resolving deltas: 100% (2361/2361), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seeds\n",
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Random seed to set. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Set the seed for general torch operations\n",
        "    torch.manual_seed(seed)\n",
        "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "jc7ail7ypKK9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_data(source: str,\n",
        "                  destination: str,\n",
        "                  remove_source: bool = True) -> Path:\n",
        "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n",
        "\n",
        "    Args:\n",
        "        source (str): A link to a zipped file containing data.\n",
        "        destination (str): A target directory to unzip data to.\n",
        "        remove_source (bool): Whether to remove the source after downloading and extracting.\n",
        "\n",
        "    Returns:\n",
        "        pathlib.Path to downloaded data.\n",
        "\n",
        "    Example usage:\n",
        "        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                      destination=\"pizza_steak_sushi\")\n",
        "    \"\"\"\n",
        "    # Setup path to data folder\n",
        "    data_path = Path(\"data/\")\n",
        "    image_path = data_path / destination\n",
        "\n",
        "    # If the image folder doesn't exist, download it and prepare it...\n",
        "    if image_path.is_dir():\n",
        "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
        "        image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Download pizza, steak, sushi data\n",
        "        target_file = Path(source).name\n",
        "        with open(data_path / target_file, \"wb\") as f:\n",
        "            request = requests.get(source)\n",
        "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
        "            f.write(request.content)\n",
        "\n",
        "        # Unzip pizza, steak, sushi data\n",
        "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
        "            print(f\"[INFO] Unzipping {target_file} data...\")\n",
        "            zip_ref.extractall(image_path)\n",
        "\n",
        "        # Remove .zip file\n",
        "        if remove_source:\n",
        "            os.remove(data_path / target_file)\n",
        "\n",
        "    return image_path\n",
        "\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYDPF4TjpNrM",
        "outputId": "3016dea2-f19f-4159-cbfd-ba31d8156e29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "def create_writer(experiment_name: str,\n",
        "                  model_name: str,\n",
        "                  extra: str=None):\n",
        "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
        "\n",
        "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
        "\n",
        "    Where timestamp is the current date in YYYY-MM-DD format.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): Name of experiment.\n",
        "        model_name (str): Name of model.\n",
        "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
        "\n",
        "    Example usage:\n",
        "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
        "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
        "                               model_name=\"effnetb2\",\n",
        "                               extra=\"5_epochs\")\n",
        "        # The above is the same as:\n",
        "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    import os\n",
        "\n",
        "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
        "\n",
        "    if extra:\n",
        "        # Create log directory path\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
        "    else:\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "\n",
        "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
        "    return SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "\n",
        "# Create a test writer\n",
        "writer = create_writer(experiment_name=\"test_experiment_name\",\n",
        "                       model_name=\"this_is_the_model_name\",\n",
        "                       extra=\"add_a_little_extra_if_you_want\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fonUeUe-pdg1",
        "outputId": "3cdc924a-ef0d-4dc1-9696-963950c1354c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Created SummaryWriter, saving to: runs/2024-01-03/test_experiment_name/this_is_the_model_name/add_a_little_extra_if_you_want...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi `create_writer` digunakan untuk membuat instance dari `SummaryWriter` dari modul TensorBoard PyTorch. Writer ini akan menyimpan log ke direktori tertentu yang dibentuk dari kombinasi `runs/timestamp/experiment_name/model_name/extra`. Timestamp adalah tanggal saat ini dalam format YYYY-MM-DD. Contoh penggunaan fungsi ini diberikan pada bagian komentar dalam blok dokumen tersebut."
      ],
      "metadata": {
        "id": "VJYEP2Ltp-qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from going_modular.going_modular.engine import train_step, test_step\n",
        "\n",
        "# Add writer parameter to train()\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device,\n",
        "          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n",
        "          ) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Stores metrics to specified writer log_dir if present.\n",
        "\n",
        "    Args:\n",
        "      model: A PyTorch model to be trained and tested.\n",
        "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "      epochs: An integer indicating how many epochs to train for.\n",
        "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "      writer: A SummaryWriter() instance to log model results to.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary of training and testing loss as well as training and\n",
        "      testing accuracy metrics. Each metric has a value in a list for\n",
        "      each epoch.\n",
        "      In the form: {train_loss: [...],\n",
        "                train_acc: [...],\n",
        "                test_loss: [...],\n",
        "                test_acc: [...]}\n",
        "      For example if training for epochs=2:\n",
        "              {train_loss: [2.0616, 1.0537],\n",
        "                train_acc: [0.3945, 0.3945],\n",
        "                test_loss: [1.2641, 1.5706],\n",
        "                test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "        ### New: Use the writer parameter to track experiments ###\n",
        "        # See if there's a writer, if so, log to it\n",
        "        if writer:\n",
        "            # Add results to SummaryWriter\n",
        "            writer.add_scalars(main_tag=\"Loss\",\n",
        "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
        "                                                \"test_loss\": test_loss},\n",
        "                               global_step=epoch)\n",
        "            writer.add_scalars(main_tag=\"Accuracy\",\n",
        "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
        "                                                \"test_acc\": test_acc},\n",
        "                               global_step=epoch)\n",
        "\n",
        "            # Close the writer\n",
        "            writer.close()\n",
        "        else:\n",
        "            pass\n",
        "    ### End new ###\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ],
      "metadata": {
        "id": "QjS9hXJhqAsE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi `train` digunakan untuk melatih dan menguji model PyTorch. Fungsi ini menggunakan metode `train_step()` dan `test_step()` untuk menjalankan model PyTorch melalui beberapa epoch, melatih dan menguji model dalam satu loop epoch yang sama. Fungsi juga menghitung, mencetak, dan menyimpan metrik evaluasi selama proses.\n",
        "\n",
        "Fungsi ini menerima parameter baru, yaitu `writer`, yang merupakan instance dari `SummaryWriter` dari modul TensorBoard PyTorch. Parameter ini digunakan untuk menyimpan hasil log ke direktori tertentu jika diberikan.\n",
        "\n",
        "Hasil dari proses pelatihan dan pengujian disimpan dalam dictionary `results`, yang berisi loss dan akurasi untuk pelatihan dan pengujian setiap epoch. Dictionary ini memiliki format:\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"train_loss\": [...],\n",
        "    \"train_acc\": [...],\n",
        "    \"test_loss\": [...],\n",
        "    \"test_acc\": [...]\n",
        "}\n"
      ],
      "metadata": {
        "id": "yk4WXsNHqRvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Download data**"
      ],
      "metadata": {
        "id": "ycCdUfXoqfZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 10 percent and 20 percent training data (if necessary)\n",
        "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                                     destination=\"pizza_steak_sushi\")\n",
        "\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVIOHhZQqUMv",
        "outputId": "fc5d8201-89cf-4c59-c414-5250bcd39608"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n",
            "[INFO] Did not find data/pizza_steak_sushi_20_percent directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi_20_percent.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode di atas digunakan untuk mengunduh data latih dengan persentase 10% dan 20% jika data tersebut belum ada di sistem. Fungsi `download_data` dipanggil dua kali, masing-masing untuk mengunduh dataset dengan persentase 10% dan 20%.\n",
        "\n",
        "- `data_10_percent_path`: Variabel ini menyimpan hasil panggilan fungsi `download_data` untuk mendapatkan data latih dengan persentase 10%.\n",
        "- `data_20_percent_path`: Variabel ini menyimpan hasil panggilan fungsi `download_data` untuk mendapatkan data latih dengan persentase 20%..\n",
        "\n",
        "Fungsi `download_data` sendiri bertanggung jawab untuk mengunduh dataset dari URL sumber dan menyimpannya di direktori tujuan yang ditentukan.\n"
      ],
      "metadata": {
        "id": "HHM8EQfxrE6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training directory paths\n",
        "train_dir_10_percent = data_10_percent_path / \"train\"\n",
        "train_dir_20_percent = data_20_percent_path / \"train\"\n",
        "\n",
        "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
        "test_dir = data_10_percent_path / \"test\"\n",
        "\n",
        "# Check the directories\n",
        "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
        "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
        "print(f\"Testing directory: {test_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8h1zBFqqkvG",
        "outputId": "f6850104-f3a4-484b-e978-27579e5b1136"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training directory 10%: data/pizza_steak_sushi/train\n",
            "Training directory 20%: data/pizza_steak_sushi_20_percent/train\n",
            "Testing directory: data/pizza_steak_sushi/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode di atas digunakan untuk menyiapkan jalur direktori dataset latih dan uji dengan persentase 10% dan 20%. Proses ini melibatkan pengaturan jalur untuk direktori latih dan uji menggunakan jalur yang diunduh sebelumnya.\n",
        "\n",
        "- `train_dir_10_percent`: Variabel ini menentukan jalur direktori latih untuk dataset dengan persentase 10%. Jalur ini dibentuk dengan menggabungkan jalur data_10_percent_path dengan subdirektori \"train\". Hasilnya adalah jalur ke direktori yang berisi dataset latih dengan persentase 10%.\n",
        "\n",
        "- `train_dir_20_percent`: Variabel ini menentukan jalur direktori latih untuk dataset dengan persentase 20%. Jalur ini dibentuk dengan menggabungkan jalur data_20_percent_path dengan subdirektori \"train\". Hasilnya adalah jalur ke direktori yang berisi dataset latih dengan persentase 20%.\n",
        "\n",
        "- `test_dir`: Variabel ini menentukan jalur direktori uji yang akan digunakan untuk kedua dataset dengan persentase 10% dan 20%. Jalur ini dibentuk dengan menggabungkan jalur data_10_percent_path dengan subdirektori \"test\". Hasilnya adalah jalur ke direktori yang berisi dataset uji yang sama untuk kedua dataset latih.\n",
        "\n",
        "Selanjutnya, kode mencetak jalur direktori latih dengan persentase 10%, jalur direktori latih dengan persentase 20%, dan jalur direktori uji."
      ],
      "metadata": {
        "id": "e-rMwT5-rSVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Create a transform to normalize data distribution to be inline with ImageNet\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Create a transform pipeline\n",
        "simple_transform = transforms.Compose([\n",
        "                                       transforms.Resize((224, 224)),\n",
        "                                       transforms.ToTensor(), # get image values between 0 & 1\n",
        "                                       normalize\n",
        "])"
      ],
      "metadata": {
        "id": "VSOSyVbFrUvA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode di atas digunakan untuk membuat transformasi pada data gambar menggunakan modul `transforms` dari PyTorch. Transformasi ini biasanya diterapkan pada dataset sebelum dimasukkan ke dalam model untuk pelatihan atau inferensi. Penjelasan dari kode tersebut sebagai berikut:\n",
        "\n",
        "- `normalize`: Variabel ini menggunakan transformasi normalisasi untuk mengubah distribusi warna gambar agar sejajar dengan ImageNet. Nilai rata-rata dan standar deviasi yang digunakan diatur sesuai dengan nilai yang telah diukur dari ImageNet dataset. Transformasi ini membantu model untuk belajar lebih baik dengan mendapatkan gambar yang memiliki distribusi warna yang serupa dengan dataset ImageNet.\n",
        "\n",
        "- `simple_transform`: Variabel ini menggunakan modul `Compose` untuk membuat sebuah pipeline transformasi yang terdiri dari beberapa tahap transformasi. Transformasi yang diterapkan melibatkan:\n",
        "  - `Resize((224, 224))`: Mengubah ukuran gambar menjadi 224x224 piksel.\n",
        "  - `ToTensor()`: Mengubah nilai piksel gambar menjadi rentang antara 0 dan 1 dan mengonversinya menjadi tensor.\n",
        "  - `normalize`: Melakukan normalisasi distribusi warna gambar sesuai dengan nilai mean dan std yang telah diatur sebelumnya.\n",
        "\n",
        "Transformasi ini bertujuan untuk mempersiapkan data gambar sehingga sesuai untuk digunakan sebagai input pada model yang telah dilatih dengan data ImageNet.\n"
      ],
      "metadata": {
        "id": "l1rs2MB0raOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Turn data into DataLoaders**"
      ],
      "metadata": {
        "id": "rhjTcErQrj4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create 10% training and test DataLoaders\n",
        "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
        "                                                                                          test_dir=test_dir,\n",
        "                                                                                          transform=simple_transform,\n",
        "                                                                                          batch_size=BATCH_SIZE)\n",
        "\n",
        "# Create 20% training and test DataLoaders\n",
        "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
        "                                                                                          test_dir=test_dir,\n",
        "                                                                                          transform=simple_transform,\n",
        "                                                                                          batch_size=BATCH_SIZE)\n",
        "\n",
        "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
        "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a-bjdgBriQ3",
        "outputId": "58d56031-cb7a-4962-c655-67460745ddc3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches of size 32 in 10 percent training data: 8\n",
            "Number of batches of size 32 in 20 percent training data: 15\n",
            "Number of batches of size 32 in testing data: 8 (all experiments will use the same test set)\n",
            "Number of classes: 3, class names: ['pizza', 'steak', 'sushi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode di atas digunakan untuk membuat DataLoaders dari dataset pelatihan dan pengujian dengan menggunakan fungsi `create_dataloaders` dari modul `data_setup`. Penjelasan dari kode tersebut sebagai berikut:\n",
        "\n",
        "- `BATCH_SIZE = 32`: Variabel ini menentukan ukuran batch yang akan digunakan dalam setiap iterasi pelatihan.\n",
        "\n",
        "- `train_dataloader_10_percent`: DataLoader yang dibuat menggunakan dataset pelatihan 10%. Menggunakan fungsi `create_dataloaders`, dengan parameter berupa direktori pelatihan 10%, direktori pengujian, transformasi data (`simple_transform`), dan ukuran batch (`BATCH_SIZE`). DataLoader ini akan digunakan untuk melatih model pada 10% dari dataset.\n",
        "\n",
        "- `train_dataloader_20_percent`: DataLoader yang dibuat menggunakan dataset pelatihan 20%. Sama seperti sebelumnya, namun kali ini menggunakan dataset pelatihan 20%.\n",
        "\n",
        "- `test_dataloader`: DataLoader yang digunakan untuk pengujian. Sama dengan DataLoader pengujian pada eksperimen sebelumnya, ini digunakan untuk evaluasi performa model pada dataset pengujian yang sama pada kedua eksperimen.\n",
        "\n",
        "- `class_names`: Variabel yang menyimpan nama kelas/kategori dari dataset. Dihasilkan oleh fungsi `create_dataloaders`.\n",
        "\n",
        "- Output yang dicetak:\n",
        "  - Jumlah batch dalam dataset pelatihan 10%.\n",
        "  - Jumlah batch dalam dataset pelatihan 20%.\n",
        "  - Jumlah batch dalam dataset pengujian (digunakan oleh kedua eksperimen).\n",
        "  - Jumlah kelas dan nama kelas dalam dataset.\n"
      ],
      "metadata": {
        "id": "l__dQb1RsSwf"
      }
    }
  ]
}